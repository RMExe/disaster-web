{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NY Time API Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools in this notebook are for the use of scraping news articles using the NY Times API. The following code will make requests to the API, scrape 1,000 of the most news recent articles, and parse the results for each article headline, snippet, and url into a DataFrame.   \n",
    "\n",
    "Please visit https://developers.nytimes.com/ before using this tool to review the NY Times API terms of service, obtain your personal NY Times developer API key (free), and research any additional information relating to the use of the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Times API Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your API key and the topic you which to search for, as a string in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''\n",
    "topic = 'fire'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function makes multiple requests to the API, pulls 1,000 of the most recent articles (10 per request) for the topic designated above. This is the maximum amount of request the API allows for, adjust the range in the function to collect a lesser amount. Output is a list of dictionaries with all article specific information contained in the json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nytimes_api_scrape(topic, api_key):\n",
    "    article_list = []\n",
    "    # adjust range to scrape less then 1,000\n",
    "    for i in range(100):\n",
    "        \n",
    "        # gives an update every 100 articles\n",
    "        if i % 10 == 0:\n",
    "            print('{} articles gathered so far'.format(i*10))\n",
    "        url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?q='+ topic + '&'+ str(i) +'&api-key='+ api_key\n",
    "        res = requests.get(url)\n",
    "        \n",
    "        # checks to see if request was a success and adds to list\n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            article_list.extend(the_json['response']['docs'])\n",
    "        \n",
    "        else:\n",
    "            print('Bad request status {}'.format(response.status_code))\n",
    "            break\n",
    "        \n",
    "        # intentionally delay requests to the server\n",
    "        time.sleep(5)\n",
    "    \n",
    "    print('You gathered {} articles about {}'.format(len(article_list), topic))    \n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** total run time of the function is about 8.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 articles gathered so far\n",
      "You gathered 30 articles about fire\n"
     ]
    }
   ],
   "source": [
    "fire_articles = nytimes_api_scrape(topic, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['web_url', 'snippet', 'lead_paragraph', 'print_page', 'blog', 'source', 'multimedia', 'headline', 'keywords', 'pub_date', 'document_type', 'news_desk', 'section_name', 'byline', 'type_of_material', '_id', 'word_count', 'score', 'uri'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_articles[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the headline, snippet, and url for each article and returns a DataFrame for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(article_list):\n",
    "    key_list = []\n",
    "    for i in range(len(fire_articles)):\n",
    "        key_list.append({\n",
    "            'headline': article_list[i]['headline']['main'],\n",
    "            'snippet': article_list[i]['snippet'],\n",
    "            'web_url': article_list[i]['web_url']\n",
    "        })\n",
    "    df = pd.DataFrame(key_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = to_df(fire_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv to use for use in other notebook\n",
    "df.to_csv('./datasets/{}'.format(topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
