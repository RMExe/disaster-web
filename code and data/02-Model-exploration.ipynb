{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I have created a workflow to build and test models, while being able to tune their parameters.  At the start, the csv is read in, and then some helper functions help create a collection of Pipelines that include a vectorizer and a classifier, then parameter grids are setup.  One of the important things here is the use of recall as a scoring metric.  Originally, I tried with accuracy and realized that this had a baseline of $\\approx 95\\%$ which was not very useful for comparing scores.  The justification for recall is making sure relative content is brought to attention, which it measures well as the best way to increase it is to reduce the amount of False Negatives the model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally was just using blizzard to make the whole process faster, as fitting tons of models to a dataset magnitudes larger would severely increase time, and realistically the parameters for most pieces should be very similar.  The next notebook covers training the model from _all_ the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_label</th>\n",
       "      <th>headline</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>snippet</th>\n",
       "      <th>web_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Blizzard Clobbers Plains and Midwest After Bla...</td>\n",
       "      <td>2019-04-11T11:22:21+0000</td>\n",
       "      <td>A powerful blizzard slammed the U.S. Plains an...</td>\n",
       "      <td>https://www.nytimes.com/reuters/2019/04/11/us/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Spring Snow Falls in New England, U.S. Midwest...</td>\n",
       "      <td>2019-04-09T01:04:40+0000</td>\n",
       "      <td>New England states enjoying the first signs of...</td>\n",
       "      <td>https://www.nytimes.com/reuters/2019/04/08/us/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Blizzard Barrels Through U.S. Great Plains, Th...</td>\n",
       "      <td>2019-04-10T06:16:34+0000</td>\n",
       "      <td>A \"bomb cyclone\" blizzard swept out of the Roc...</td>\n",
       "      <td>https://www.nytimes.com/reuters/2019/04/10/us/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Blizzard Forces Closure of Some U.S. Grain Pro...</td>\n",
       "      <td>2019-04-11T16:41:16+0000</td>\n",
       "      <td>A second \"bomb cyclone\" blizzard hitting the U...</td>\n",
       "      <td>https://www.nytimes.com/reuters/2019/04/11/us/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>No, Winter Isn’t Over. Hitting the Plains: A F...</td>\n",
       "      <td>2019-03-14T19:16:18+0000</td>\n",
       "      <td>Rain, melting snow and frozen ground lifted ri...</td>\n",
       "      <td>https://www.nytimes.com/2019/03/14/us/bomb-cyc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_label                                           headline  \\\n",
       "0        0  Blizzard Clobbers Plains and Midwest After Bla...   \n",
       "1        0  Spring Snow Falls in New England, U.S. Midwest...   \n",
       "2        1  Blizzard Barrels Through U.S. Great Plains, Th...   \n",
       "3        1  Blizzard Forces Closure of Some U.S. Grain Pro...   \n",
       "4        0  No, Winter Isn’t Over. Hitting the Plains: A F...   \n",
       "\n",
       "                   pub_date  \\\n",
       "0  2019-04-11T11:22:21+0000   \n",
       "1  2019-04-09T01:04:40+0000   \n",
       "2  2019-04-10T06:16:34+0000   \n",
       "3  2019-04-11T16:41:16+0000   \n",
       "4  2019-03-14T19:16:18+0000   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  A powerful blizzard slammed the U.S. Plains an...   \n",
       "1  New England states enjoying the first signs of...   \n",
       "2  A \"bomb cyclone\" blizzard swept out of the Roc...   \n",
       "3  A second \"bomb cyclone\" blizzard hitting the U...   \n",
       "4  Rain, melting snow and frozen ground lifted ri...   \n",
       "\n",
       "                                             web_url  \n",
       "0  https://www.nytimes.com/reuters/2019/04/11/us/...  \n",
       "1  https://www.nytimes.com/reuters/2019/04/08/us/...  \n",
       "2  https://www.nytimes.com/reuters/2019/04/10/us/...  \n",
       "3  https://www.nytimes.com/reuters/2019/04/11/us/...  \n",
       "4  https://www.nytimes.com/2019/03/14/us/bomb-cyc...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blizzard_df = pd.read_csv(\"./datasets/blizzard.csv\",index_col=1)\n",
    "blizzard_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = blizzard_df[\"headline\"]\n",
    "y = blizzard_df[\"y_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,\n",
    "                                                 y,\n",
    "                                                 stratify=y,\n",
    "                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextGet(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass #do nothing to initialize\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        return X[\"headline\"]\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.transform(X,y)\n",
    "\n",
    "#This class definition was given as an example in SKLearn's docs here: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to construct the Pipelines and their `param_grids`.  Note the use of feature union to branch each pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_constructor(classifiers):\n",
    "    pipes = []\n",
    "    for name,Classifier in classifiers:\n",
    "\n",
    "        pipes.append(Pipeline([\n",
    "            (\"cvec\",CountVectorizer(tokenizer=LemmaTokenizer())),\n",
    "            (name,Classifier())\n",
    "        ]))\n",
    "\n",
    "        pipes.append(Pipeline([\n",
    "            (\"tvec\",TfidfVectorizer(tokenizer=LemmaTokenizer())),\n",
    "            (name,Classifier())\n",
    "        ]))\n",
    "    return pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell controls what models I will be testing.  From this list the functon defined above will automatically generate the combinations of the vectorizers and the classifiers, and then spit it out as a list of pipes.  This list would be sufficient to grid search over, but I did unpack them into individual names just to show the combinations a little more explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [(\"logclf\",LogisticRegression),\n",
    "               (\"nb\",MultinomialNB),\n",
    "               (\"gboost\",GradientBoostingClassifier),\n",
    "               (\"ada\",AdaBoostClassifier),\n",
    "               (\"bag\",BaggingClassifier)\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes = pipe_constructor(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Param Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "cvec_grid = {\"cvec__stop_words\" : [stopWords],\n",
    "             \"cvec__ngram_range\": [(1,1),(1,2),(1,3)],\n",
    "             \"cvec__min_df\" : [1,5,10,15],\n",
    "             \"cvec__max_df\" : [.9,.95,1.0],\n",
    "             \"cvec__max_features\" : [1000,2000,None]}\n",
    "tvec_grid = {\"tvec__stop_words\" : [stopWords],\n",
    "             \"tvec__ngram_range\": [(1,1),(1,2),(1,3)],\n",
    "             \"tvec__min_df\" : [1,5,10,15],\n",
    "             \"tvec__max_df\" : [.9,.95,1.0],\n",
    "             \"tvec__max_features\" : [1000,2000,None]}\n",
    "# knn_grid = {\"knn__n_neighbors\":[5]}\n",
    "log_grid = {\"logclf__C\" : np.linspace(0.1,1.0,5),\n",
    "            \"logclf__solver\": [\"liblinear\",\"lbfgs\"],\n",
    "            \"logclf__class_weight\" : [\"balanced\"]}\n",
    "nb_grid = {\"nb__alpha\": np.linspace(0.15,0.35,20)}\n",
    "# dt_grid = {\"dt__max_depth\" : [None,200,300],\n",
    "#           \"dt__max_features\" : [\"auto\",None]}\n",
    "# gboost_grid = {\"gboost__max_depth\" : [2,3,4]}\n",
    "# ada_grid = {\"ada__learning_rate\" : np.linspace(0.1,1,5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop essentially zips all these `param_grids` together.  Dictionaries are mutable so `copy.copy()` is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = []\n",
    "cl_grids = [log_grid,nb_grid]\n",
    "for cl_grid in cl_grids:\n",
    "    new_cvec = copy.copy(cvec_grid)\n",
    "    new_cvec.update(cl_grid)\n",
    "    new_tvec = copy.copy(tvec_grid)\n",
    "    new_tvec.update(cl_grid)\n",
    "    param_grids.append(new_cvec)\n",
    "    param_grids.append(new_tvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now by using `zip()` to correctly pair the pipe to its corresponding `param_grid`, we can gridsearch a great amount of models at once, then we will take a look at their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1080 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   50.5s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3240 out of 3240 | elapsed:  6.0min finished\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1080 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3240 out of 3240 | elapsed:  5.4min finished\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2160 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   35.7s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 11.4min finished\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2160 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 12.4min finished\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "grids = []\n",
    "for pipe,param_grid in zip(pipes,param_grids):\n",
    "    grid = GridSearchCV(pipe,\n",
    "                        param_grid=param_grid,\n",
    "                        #RECALL is important here, because of the imbalanced classes.  f1_score presents another viable path for optimization.\n",
    "                        scoring=\"recall\",\n",
    "                        cv=3,\n",
    "                        n_jobs=-1,verbose=2)\n",
    "    grid.fit(X_train,y_train)\n",
    "    grids.append(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.954\n",
       "1    0.046\n",
       "Name: y_label, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blizzard_df.y_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a formatted output of the model scores on the training data and the testing data.  The testing data score is the most important, because it represents the approximation of performance on novel, unseen data.  The training data is included because it can help expose bias and variance of the models at a glance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_scores(model, details, X_train,X_test,y_train,y_test):\n",
    "    #get scores\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    train_score = recall_score(y_train,y_pred_train)\n",
    "    test_score = recall_score(y_test,y_pred_test)\n",
    "    #output\n",
    "    print(f\"{details['name']} with {details['processor']}'s Recall Scores:\")\n",
    "    print(\"=====================================================\")\n",
    "    print(f\"Performance on training data: {train_score*100}%\")\n",
    "    print(f\"Performance on training data: {test_score*100}%\")\n",
    "    print(\"=====================================================\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardcoded lookup dictionary for formatting purposes\n",
    "lookup = {\n",
    "    \"knn\" : \"k-Nearest Neighbors\",\n",
    "    \"logclf\" : \"Logistic Regression\",\n",
    "    \"nb\" : \"Multinomial Naïve Bayes\",\n",
    "    \"svc\" : \"Support Vector Classifier\",\n",
    "    \"dt\" : \"Decision Tree Classifier\",\n",
    "    \"gboost\" : \"Gradient Boosting Classifier\",\n",
    "    \"bag\" : \"Bagging Classifier\",\n",
    "    \"ada\" : \"AdaBoost Classifier\",\n",
    "    \"cvec\" : \"Count Vectorizer\",\n",
    "    \"tvec\" : \"TfidVectorizer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_dicts = []\n",
    "for model in grids:\n",
    "    details = {}\n",
    "    details[\"name\"] = lookup[model.steps[1][0]]\n",
    "    details[\"processor\"] = lookup[model.steps[0][0]]\n",
    "    detail_dicts.append(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Logistic Regression', 'processor': 'Count Vectorizer'},\n",
       " {'name': 'Logistic Regression', 'processor': 'TfidVectorizer'},\n",
       " {'name': 'Multinomial Naïve Bayes', 'processor': 'Count Vectorizer'},\n",
       " {'name': 'Multinomial Naïve Bayes', 'processor': 'TfidVectorizer'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detail_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with Count Vectorizer's Recall Scores:\n",
      "=====================================================\n",
      "Performance on training data: 88.57142857142857%\n",
      "Performance on training data: 81.81818181818183%\n",
      "=====================================================\n",
      "\n",
      "Logistic Regression with TfidVectorizer's Recall Scores:\n",
      "=====================================================\n",
      "Performance on training data: 82.85714285714286%\n",
      "Performance on training data: 72.72727272727273%\n",
      "=====================================================\n",
      "\n",
      "Multinomial Naïve Bayes with Count Vectorizer's Recall Scores:\n",
      "=====================================================\n",
      "Performance on training data: 94.28571428571428%\n",
      "Performance on training data: 63.63636363636363%\n",
      "=====================================================\n",
      "\n",
      "Multinomial Naïve Bayes with TfidVectorizer's Recall Scores:\n",
      "=====================================================\n",
      "Performance on training data: 25.71428571428571%\n",
      "Performance on training data: 18.181818181818183%\n",
      "=====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, details in zip(grids,detail_dicts):\n",
    "    formatted_scores(model,details,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permanent Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section just serializes the details into a file on the hard drive to make it easier to reconstruct later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for g in grids:\n",
    "    # Unpack the tuples\n",
    "    name = g.steps[1][0]\n",
    "    model = g.steps[1][1]\n",
    "    \n",
    "    #get the parameters for the most optimized version from the gridsearch\n",
    "    model_dict = model.get_params()\n",
    "    model_dict[\"name\"] = name\n",
    "    \n",
    "    # form a sub dictionary\n",
    "    vect_dict = g.steps[0][1].get_params()\n",
    "    \n",
    "    #remove pieces that cant be serialized and arent necessary for reconstruction\n",
    "    del vect_dict[\"tokenizer\"]\n",
    "    del vect_dict[\"dtype\"]\n",
    "    \n",
    "    #turn into a serializible data structrue\n",
    "    vect_dict[\"stop_words\"] = list(vect_dict[\"stop_words\"])\n",
    "    model_dict[\"vect_details\"] = vect_dict\n",
    "    \n",
    "    #attach this as a sub-dictionary\n",
    "    models.append(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/opt_model_params.json\",\"w\") as fp:\n",
    "    json.dump(models,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redundant with a `random_state` set in theory, but if that compatibility breaks between versions this should help retain reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_train.to_csv('./datasets/blizzard_X_train.csv',index=False)\n",
    "X_test.to_csv('./datasets/blizzard_X_test.csv',index=False)\n",
    "y_train.to_csv('./datasets/blizzard_y_train.csv',index=False)\n",
    "y_test.to_csv('./datasets/blizzard_y_test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
